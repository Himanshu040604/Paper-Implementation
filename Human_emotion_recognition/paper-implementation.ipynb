{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wasn't able to implement the whole thing due to gpu restriction but checked with toy data for the clarrification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:51:18.723390Z",
     "iopub.status.busy": "2026-01-10T06:51:18.723013Z",
     "iopub.status.idle": "2026-01-10T06:51:18.731227Z",
     "shell.execute_reply": "2026-01-10T06:51:18.730463Z",
     "shell.execute_reply.started": "2026-01-10T06:51:18.723360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pywt\n",
    "from scipy.interpolate import griddata\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "FS = 128     \n",
    "WIN_SEC = 2       \n",
    "OVERLAP = 0.5     \n",
    "WAVELET = \"db4\"\n",
    "DWT_LEVEL = 5\n",
    "\n",
    "EEG_COORDS = np.array([\n",
    "    [-0.5,  1.0],   \n",
    "    [-1.0,  0.5],   \n",
    "    [-0.5,  0.5],  \n",
    "    [-0.8,  0.0],  \n",
    "    [-1.0,  0.0],   \n",
    "    [-0.8, -0.5],   \n",
    "    [-0.5, -1.0],  \n",
    "    [ 0.5, -1.0],   \n",
    "    [ 0.8, -0.5],  \n",
    "    [ 1.0,  0.0],   \n",
    "    [ 0.5,  0.5],   \n",
    "    [ 1.0,  0.5],   \n",
    "    [ 0.5,  1.0],   \n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "def load_amigos_mat(mat_path):\n",
    "    mat = sio.loadmat(mat_path, squeeze_me=False)\n",
    "    joined = mat.get(\"joined_data\")\n",
    "    labels = mat.get(\"labels_selfassessment\")\n",
    "    if joined is None or labels is None:\n",
    "        raise KeyError(f\"joined_data or labels_selfassessment not found in {mat_path}\")\n",
    "    eeg_trials = []\n",
    "    label_trials = []\n",
    "    for i in range(joined.shape[1]):\n",
    "        signal = joined[0, i]       \n",
    "        label = labels[0, i]        \n",
    "        eeg = signal[:, :14].T      \n",
    "        eeg_trials.append(eeg.astype(np.float32))\n",
    "        label_trials.append(np.asarray(label, dtype=np.float32))\n",
    "    return eeg_trials, label_trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:51:22.055607Z",
     "iopub.status.busy": "2026-01-10T06:51:22.055006Z",
     "iopub.status.idle": "2026-01-10T06:51:22.083894Z",
     "shell.execute_reply": "2026-01-10T06:51:22.083018Z",
     "shell.execute_reply.started": "2026-01-10T06:51:22.055578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def window_eeg(eeg_list, label_list, fs=FS, win_sec=WIN_SEC, overlap=OVERLAP):\n",
    "    win_len = int(fs * win_sec)\n",
    "    step = max(1, int(win_len * (1 - overlap)))\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for signal, label in zip(eeg_list, label_list):\n",
    "        for start in range(0, signal.shape[1] - win_len + 1, step):\n",
    "            window = signal[:, start:start + win_len]\n",
    "            segments.append(window)\n",
    "            labels.append(label[:2]) \n",
    "    return segments, labels\n",
    "\n",
    "def dwt_band_energies(window, wavelet=WAVELET, level=DWT_LEVEL):\n",
    "    feats = []\n",
    "    for ch in window:\n",
    "        coeffs = pywt.wavedec(ch, wavelet, level=level)\n",
    "        band_energy = [np.mean(c ** 2) for c in coeffs[:5]]\n",
    "        feats.append(band_energy)\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def create_topomaps(band_matrix, coords=EEG_COORDS, grid_size=32):\n",
    "    n_channels, n_bands = band_matrix.shape\n",
    "    m = min(coords.shape[0], n_channels)\n",
    "    xs, ys = coords[:m, 0], coords[:m, 1]\n",
    "    band_matrix = band_matrix[:m] \n",
    "    grid_x, grid_y = np.mgrid[\n",
    "        np.min(xs):np.max(xs):complex(grid_size),\n",
    "        np.min(ys):np.max(ys):complex(grid_size)\n",
    "    ]\n",
    "    topo = np.zeros((n_bands, grid_size, grid_size), dtype=np.float32)\n",
    "    for b in range(n_bands):\n",
    "        values = band_matrix[:, b]\n",
    "        interp = griddata((xs, ys), values, (grid_x, grid_y),\n",
    "                          method='linear', fill_value=np.mean(values))\n",
    "        interp_min, interp_max = interp.min(), interp.max()\n",
    "        topo[b] = (interp - interp_min) / (interp_max - interp_min + 1e-8)\n",
    "    return topo\n",
    "\n",
    "\n",
    "class AMIGOSDataset(Dataset):\n",
    "    def __init__(self, mat_files, grid_size=32, transform=None):\n",
    "        self.samples = []\n",
    "        self.grid_size = grid_size\n",
    "        self.transform = transform\n",
    "        for mfile in mat_files:\n",
    "            eeg_trials, label_trials = load_amigos_mat(mfile)\n",
    "            segments, labels = window_eeg(eeg_trials, label_trials, fs=FS, win_sec=WIN_SEC, overlap=OVERLAP)\n",
    "            for seg, lab in zip(segments, labels):\n",
    "                seg_norm = seg / (np.max(np.abs(seg), axis=1, keepdims=True) + 1e-8)\n",
    "                band_mat = dwt_band_energies(seg_norm, wavelet=WAVELET, level=DWT_LEVEL)\n",
    "                topo = create_topomaps(band_mat, coords=EEG_COORDS, grid_size=grid_size)\n",
    "                self.samples.append((topo.astype(np.float32), lab.astype(np.float32)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        topo, label = self.samples[idx]\n",
    "        topo_tensor = torch.tensor(topo, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            topo_tensor = self.transform(topo_tensor)\n",
    "        return { 'topo': topo_tensor, 'label': label_tensor }\n",
    "\n",
    "\n",
    "class DCAB(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False)\n",
    "        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        ca = self.sigmoid(avg_out + max_out)\n",
    "        x = x * ca\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        sa = self.sigmoid(self.spatial_conv(torch.cat([avg_pool, max_pool], dim=1)))\n",
    "        x = x * sa\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPUMB(nn.Module):\n",
    "    def __init__(self, channels, kernels=(3, 5, 7)):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, kernel_size=k, padding=k // 2, groups=channels, bias=False),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.GELU()\n",
    "            ) for k in kernels\n",
    "        ])\n",
    "        self.pointwise = nn.Conv2d(channels * len(kernels), channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [b(x) for b in self.branches]\n",
    "        concat = torch.cat(outs, dim=1)\n",
    "        out = self.pointwise(concat)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "class CNNEncoderLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=1, num_channels=channels)\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.dcab = DCAB(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.norm(x)\n",
    "        y = self.conv(y)\n",
    "        y = self.act(y)\n",
    "        y = self.dcab(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=5, embed_dim=768, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.pre_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, groups=embed_dim, bias=False)\n",
    "        )\n",
    "        self.proj = nn.Conv2d(embed_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_conv(x)\n",
    "        x = self.proj(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        cls_tokens = self.class_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ffn(x)\n",
    "        x = x + ff_out\n",
    "        x = self.norm2(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, in_channels=5, embed_dim=768, num_classes=2, num_encoder_blocks=4, num_dpumb_blocks=3, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels, embed_dim, patch_size)\n",
    "        blocks = []\n",
    "        total_blocks = num_encoder_blocks + num_dpumb_blocks\n",
    "        for i in range(total_blocks):\n",
    "            if i % 2 == 0:\n",
    "                blocks.append(CNNEncoderLayer(embed_dim))\n",
    "            else:\n",
    "                blocks.append(DPUMB(embed_dim))\n",
    "        self.encoder = nn.Sequential(*blocks)\n",
    "        self.pool = AttentionPool(embed_dim, num_heads=8)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.patch_embed(x)\n",
    "        tokens = tokens.transpose(1, 2)\n",
    "        N = tokens.size(2)\n",
    "        H_W = int(np.sqrt(N))\n",
    "        x2d = tokens.view(tokens.size(0), tokens.size(1), H_W, H_W)\n",
    "        features = self.encoder(x2d)\n",
    "        B, C, H, W = features.shape\n",
    "        tokens2 = features.flatten(2).transpose(1, 2)\n",
    "        pooled = self.pool(tokens2)\n",
    "        return self.head(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for single file dont run this if you want for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:42:39.733675Z",
     "iopub.status.busy": "2026-01-10T06:42:39.733333Z",
     "iopub.status.idle": "2026-01-10T06:42:48.231852Z",
     "shell.execute_reply": "2026-01-10T06:42:48.230946Z",
     "shell.execute_reply.started": "2026-01-10T06:42:39.733648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1442 samples from 1 mat file.\n",
      "logits shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "#for toy data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "    all_mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "    mat_files = [all_mat_files[0]] if all_mat_files else []\n",
    "\n",
    "    if mat_files:\n",
    "        dataset = AMIGOSDataset(mat_files, grid_size=32)\n",
    "        print(f\"Loaded {len(dataset)} samples from 1 mat file.\")\n",
    "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "        model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "        for batch in loader:\n",
    "            inputs = batch['topo']\n",
    "            labels = batch['label']\n",
    "            logits = model(inputs)\n",
    "            print(\"logits shape:\", logits.shape)\n",
    "            break\n",
    "    else:\n",
    "        print(\"No .mat files found in\", data_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:43:30.827567Z",
     "iopub.status.busy": "2026-01-10T06:43:30.826925Z",
     "iopub.status.idle": "2026-01-10T06:43:39.139897Z",
     "shell.execute_reply": "2026-01-10T06:43:39.139265Z",
     "shell.execute_reply.started": "2026-01-10T06:43:30.827537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1442 samples from 1 mat file.\n",
      "logits shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "    all_mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "    mat_files = [all_mat_files[0]] if all_mat_files else []\n",
    "    if mat_files:\n",
    "        dataset = AMIGOSDataset(mat_files, grid_size=32) \n",
    "        print(f\"Loaded {len(dataset)} samples from 1 mat file.\")\n",
    "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "        model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "        # Check a single forward pass\n",
    "        for batch in loader:\n",
    "            inputs = batch['topo']\n",
    "            labels = batch['label']\n",
    "            logits = model(inputs)\n",
    "            print(\"logits shape:\", logits.shape)\n",
    "            break\n",
    "    else:\n",
    "        print(\"No .mat files found in\", data_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T23:04:59.354394Z",
     "iopub.status.busy": "2026-01-09T23:04:59.353688Z",
     "iopub.status.idle": "2026-01-09T23:14:38.333774Z",
     "shell.execute_reply": "2026-01-09T23:14:38.332940Z",
     "shell.execute_reply.started": "2026-01-09T23:04:59.354368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 129366 samples from 23 mat files.\n",
      "logits shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "    mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "    if mat_files:\n",
    "        dataset = AMIGOSDataset(mat_files, grid_size=32)\n",
    "        print(f\"Loaded {len(dataset)} samples from {len(mat_files)} mat files.\")\n",
    "        loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "        model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "        for batch in loader:\n",
    "            inputs = batch['topo'] \n",
    "            labels = batch['label'] \n",
    "            logits = model(inputs)\n",
    "            print(\"logits shape:\", logits.shape)\n",
    "            break\n",
    "    else:\n",
    "        print(\"No .mat files found in\", data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:14:04.472288Z",
     "iopub.status.busy": "2026-01-10T07:14:04.471893Z",
     "iopub.status.idle": "2026-01-10T07:15:50.694764Z",
     "shell.execute_reply": "2026-01-10T07:15:50.693869Z",
     "shell.execute_reply.started": "2026-01-10T07:14:04.472259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6200\n",
      "Epoch 2: Train Loss = 0.5645\n",
      "Epoch 3: Train Loss = 0.5439\n",
      "Training‑set accuracies – Valence: 0.797, Arousal: 0.707\n",
      "Trained model weights saved to: trained_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def extract_binary_labels(label_tensor, threshold=5.0):\n",
    "    # If labels are shaped (B, 1, 12), flatten to (B, 12)\n",
    "    if label_tensor.dim() > 2:\n",
    "        label_tensor = label_tensor.view(label_tensor.size(0), -1)\n",
    "    labels = label_tensor[:, :2]\n",
    "    return (labels >= threshold).float()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "\n",
    "    all_mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "    mat_files = [all_mat_files[0]] if all_mat_files else []\n",
    "    if not mat_files:\n",
    "        raise FileNotFoundError(f\"No .mat files found in {data_root}\")\n",
    "\n",
    "    # Build the dataset and DataLoader\n",
    "    dataset = AMIGOSDataset(mat_files, grid_size=32)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"topo\"].to(device)            \n",
    "            raw_labels = batch[\"label\"].to(device)       \n",
    "            labels = extract_binary_labels(raw_labels)    \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)                        \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        avg_loss = running_loss / len(loader.dataset)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct_v = correct_a = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"topo\"].to(device)\n",
    "            raw_labels = batch[\"label\"].to(device)\n",
    "            labels = extract_binary_labels(raw_labels)\n",
    "            logits = model(inputs)\n",
    "            preds = (logits > 0).float()\n",
    "            correct_v += (preds[:, 0] == labels[:, 0]).sum().item()\n",
    "            correct_a += (preds[:, 1] == labels[:, 1]).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Training‑set accuracies – Valence: {correct_v/total:.3f}, Arousal: {correct_a/total:.3f}\")\n",
    "\n",
    "    save_path = \"trained_weights.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Trained model weights saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T23:24:06.257404Z",
     "iopub.status.busy": "2026-01-09T23:24:06.256835Z",
     "iopub.status.idle": "2026-01-09T23:33:49.955647Z",
     "shell.execute_reply": "2026-01-09T23:33:49.954981Z",
     "shell.execute_reply.started": "2026-01-09T23:24:06.257378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subjects: 90412 samples\n",
      "Val subjects:   18756 samples\n",
      "Test subjects:  20198 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for the whole file run this\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def subject_wise_split(mat_files, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "    n_subjects = len(mat_files)\n",
    "    subject_ids = list(range(n_subjects))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(subject_ids)\n",
    "\n",
    "    n_train = int(n_subjects * train_ratio)\n",
    "    n_val   = int(n_subjects * val_ratio)\n",
    "\n",
    "    train_subjects = subject_ids[:n_train]\n",
    "    val_subjects   = subject_ids[n_train:n_train + n_val]\n",
    "    test_subjects  = subject_ids[n_train + n_val:]\n",
    "\n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    sample_offset = 0\n",
    "    for subj_idx, mfile in enumerate(mat_files):\n",
    "        eeg_trials, label_trials = load_amigos_mat(mfile)\n",
    "        segments, labels = window_eeg(eeg_trials, label_trials, fs=FS, win_sec=WIN_SEC, overlap=OVERLAP)\n",
    "        num_samples = len(segments)\n",
    "        idx_range = list(range(sample_offset, sample_offset + num_samples))\n",
    "\n",
    "        if subj_idx in train_subjects:\n",
    "            train_indices.extend(idx_range)\n",
    "        elif subj_idx in val_subjects:\n",
    "            val_indices.extend(idx_range)\n",
    "        else:\n",
    "            test_indices.extend(idx_range)\n",
    "\n",
    "        sample_offset += num_samples\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "\n",
    "dataset = AMIGOSDataset(mat_files, grid_size=32)\n",
    "\n",
    "train_idx, val_idx, test_idx = subject_wise_split(mat_files)\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "test_ds  = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train subjects: {len(train_idx)} samples\")\n",
    "print(f\"Val subjects:   {len(val_idx)} samples\")\n",
    "print(f\"Test subjects:  {len(test_idx)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def extract_binary_labels(label_tensor, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Flatten the input label tensor and convert the first two values\n",
    "    (valence and arousal) to binary (0/1) based on the threshold.\n",
    "    \"\"\"\n",
    "    if label_tensor.dim() > 2:\n",
    "        label_tensor = label_tensor.view(label_tensor.size(0), -1)\n",
    "    labels = label_tensor[:, :2]\n",
    "    return (labels >= threshold).float()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "\n",
    "    # Load all .mat files\n",
    "    mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "    if not mat_files:\n",
    "        raise FileNotFoundError(f\"No .mat files found in {data_root}\")\n",
    "\n",
    "    # Build the dataset and DataLoader using all files\n",
    "    dataset = AMIGOSDataset(mat_files, grid_size=32)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Set up model, loss, optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training \n",
    "    num_epochs = 5\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"topo\"].to(device)          \n",
    "            raw_labels = batch[\"label\"].to(device)       \n",
    "            labels = extract_binary_labels(raw_labels)    \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)                        \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        avg_loss = running_loss / len(loader.dataset)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct_v = correct_a = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"topo\"].to(device)\n",
    "            raw_labels = batch[\"label\"].to(device)\n",
    "            labels = extract_binary_labels(raw_labels)\n",
    "            logits = model(inputs)\n",
    "            preds = (logits > 0).float()\n",
    "            correct_v += (preds[:, 0] == labels[:, 0]).sum().item()\n",
    "            correct_a += (preds[:, 1] == labels[:, 1]).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Training‑set accuracies – Valence: {correct_v/total:.3f}, Arousal: {correct_a/total:.3f}\")\n",
    "\n",
    "    save_path = \"trained_weights.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Trained model weights saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:32:34.951040Z",
     "iopub.status.busy": "2026-01-10T07:32:34.950517Z",
     "iopub.status.idle": "2026-01-10T07:32:45.950948Z",
     "shell.execute_reply": "2026-01-10T07:32:45.950157Z",
     "shell.execute_reply.started": "2026-01-10T07:32:34.951009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-file evaluation – Valence: 0.797, Arousal: 0.707\n"
     ]
    }
   ],
   "source": [
    "#testing for a single file\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def extract_binary_labels(label_tensor, threshold=5.0):\n",
    "    if label_tensor.dim() > 2:\n",
    "        label_tensor = label_tensor.view(label_tensor.size(0), -1)\n",
    "    labels = label_tensor[:, :2]\n",
    "    return (labels >= threshold).float()\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct_v = correct_a = total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch['topo'].to(device)\n",
    "            raw_labels = batch['label'].to(device)\n",
    "            labels = extract_binary_labels(raw_labels)\n",
    "            logits = model(inputs)\n",
    "            preds = (logits > 0).float()\n",
    "            correct_v += (preds[:, 0] == labels[:, 0]).sum().item()\n",
    "            correct_a += (preds[:, 1] == labels[:, 1]).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return (\n",
    "        correct_v / total if total > 0 else 0.0,\n",
    "        correct_a / total if total > 0 else 0.0\n",
    "    )\n",
    "\n",
    "data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "weights_path = \"/kaggle/working/trained_weights.pth\"\n",
    "\n",
    "all_mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "single_file = [all_mat_files[0]] if all_mat_files else []\n",
    "if not single_file:\n",
    "    raise FileNotFoundError(f\"No .mat files found in {data_root}\")\n",
    "\n",
    "single_dataset = AMIGOSDataset(single_file, grid_size=32)\n",
    "single_loader = DataLoader(single_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "val_acc, arousal_acc = evaluate_model(model, single_loader, device)\n",
    "print(f\"Single-file evaluation – Valence: {val_acc:.3f}, Arousal: {arousal_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/amigos/AMIGOS DATASET\"\n",
    "weights_path = \"/kaggle/working/trained_weights.pth\"\n",
    "\n",
    "all_mat_files = sorted(glob.glob(os.path.join(data_root, \"*.mat\")))\n",
    "if not all_mat_files:\n",
    "    raise FileNotFoundError(f\"No .mat files found in {data_root}\")\n",
    "\n",
    "full_dataset = AMIGOSDataset(all_mat_files, grid_size=32)\n",
    "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionRecognitionModel(in_channels=5, embed_dim=768, num_classes=2)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "val_acc, arousal_acc = evaluate_model(model, full_loader, device)\n",
    "print(f\"Full-dataset evaluation – Valence: {val_acc:.3f}, Arousal: {arousal_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9229512,
     "sourceId": 14449307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
